{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72a5cf8",
   "metadata": {},
   "source": [
    "# BMI 203: Step 3: Make Your Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f39ac",
   "metadata": {},
   "source": [
    "This Jupyter notebook classifies Rap1 transcription factors using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54fb5d",
   "metadata": {},
   "source": [
    "### Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03edb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import nn, preprocess, io\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4991b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f8d1f",
   "metadata": {},
   "source": [
    "Initalize seed for consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870c58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ad945",
   "metadata": {},
   "source": [
    "### Use read functions from io.py to read in positive and negative motif examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f15ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = io.read_text_file(\"data/rap1-lieb-positives.txt\")\n",
    "neg = io.read_fasta_file(\"data/yeast-upstream-1k-negative.fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae0845a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ef567",
   "metadata": {},
   "source": [
    "Determine lengths of positive and negative sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4de9e84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "for seq in range(5):\n",
    "    print( len(pos[seq]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ac8798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for seq in range(5):\n",
    "    print( len(neg[seq]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863e338",
   "metadata": {},
   "source": [
    "#### Make lengths of negative sequences equal those of positive sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207f225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neg = []\n",
    "for seq in neg:\n",
    "    len_17 = seq[:17]\n",
    "    new_neg += [len_17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12983477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "for seq in range(5):\n",
    "    print( len(new_neg[seq]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035c93a",
   "metadata": {},
   "source": [
    "Combine positve and shortened negative sequences into one list and generate corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fea7611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = list(pos) + list(new_neg)\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed480a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [True] * len(pos) + [False] * len(new_neg)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4e207",
   "metadata": {},
   "source": [
    "### Balance classes using sample_seq function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb589197",
   "metadata": {},
   "source": [
    "My sampling method with replacement defines two arrays: one of the positive and one of the negative sequence indices of the concatenated list of sequences. I then perform a \"coin-flip\", and randomly generate a float value from 0 to 1.0 from a uniform distribution, then evaluate if that value is above or below 0.5. For each sequence in the concatenated list of sequences, sample a posititive sequence if value is below 0.5, otherwise sample a negative sequence. Ultimately, this should yield a sampling of sequences with relatively balanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a40ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_seqs, sampled_labels = preprocess.sample_seqs(seqs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c74fca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0cafbde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3c023",
   "metadata": {},
   "source": [
    "Check to see if classes are relatively balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7b74bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labs = []\n",
    "neg_labs = []\n",
    "for lab in sampled_labels:\n",
    "    if lab == True:\n",
    "        pos_labs += [lab]\n",
    "    else:\n",
    "        neg_labs += [lab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83f57962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6042ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1669"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68774e80",
   "metadata": {},
   "source": [
    "### One-hot encode sampled labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e85c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sampled = preprocess.one_hot_encode_seqs(sampled_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f89d3860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_sampled[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df5e703e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( one_hot_sampled[0] ) == 17 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68aa24d",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a21e06",
   "metadata": {},
   "source": [
    "Split the data into 75% training, and 25% testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6899bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_sampled, one_hot_sampled, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03f060d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2475, 68)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27db7a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(825, 68)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "308d23e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2abd777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053c9d1",
   "metadata": {},
   "source": [
    "### Determine best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974dd800",
   "metadata": {},
   "source": [
    "I generate my classifier using sigmoid as the activation function and BCE as the loss function since the output is binary and the task is akin to logistic regression. Here I test a series of hyperparameters in my classifer to determined the optimal learning rate, batch size, and number of epochs. The architecture of my classifer has 3 layers total (input layer, two hidden layers, and binary output layer). This means my neural network should be 68x34x17x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6bf095f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize nn architecture.\n",
    "nn_arch = [{'input_dim': 68, 'output_dim': 34, 'activation': 'sigmoid'},\n",
    "           {'input_dim': 34, 'output_dim': 17, 'activation': 'sigmoid'},\n",
    "           {'input_dim': 17, 'output_dim': 1, 'activation': 'sigmoid'}\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create series of hyperparameters to test. Testing less due to computation time requirement.\n",
    "lrs = [0.001, 0.0001, 0.00001]\n",
    "batch_sizes = [10, 25, 50]\n",
    "epochs = [100, 200, 500]\n",
    "\n",
    "# Initialize list losses and parameters test runs.\n",
    "losses = []\n",
    "test_runs = []\n",
    "\n",
    "# Test hyperparameters in series.\n",
    "for lr in lrs:\n",
    "    for bs in batch_sizes:\n",
    "        for e in epochs:\n",
    "            # Generate autoenconder with current hyperparameters.\n",
    "            test_classifer = nn.NeuralNetwork(nn_arch,\n",
    "                                              lr = lr,\n",
    "                                              batch_size = bs,\n",
    "                                              epochs = e,\n",
    "                                              seed = seed,\n",
    "                                              loss_function = \"bce\")\n",
    "            loss_train, loss_val = test_classifer.fit(X_train, y_train, X_test, y_test) # Train classifier.\n",
    "            min_loss = min(loss_val) # Add minimal loss value to store best final model.\n",
    "            test_runs.append([lr, bs, e, min_loss]) # Append list of hyperparameters and loss to full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48118a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of potential optimal hyperparameters to dataframe for easy viewing.\n",
    "optimal_df = pd.DataFrame(test_runs)\n",
    "optimal_df.columns = ['Learning Rate', 'Batch Size', 'Epochs', 'Validation BCE']\n",
    "optimal_df.sort_values('Validation MSE').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640a99e",
   "metadata": {},
   "source": [
    "#### The best combination of hyperparameters that yield the minimum validation mse loss are:\n",
    "<br>\n",
    "Learning Rate: 0.0001\n",
    "<br>\n",
    "Batch Size: 10\n",
    "<br>\n",
    "Epochs: 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b4041",
   "metadata": {},
   "source": [
    "### Plot training and validation loss by epoch for best network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30796fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot training and validation losses.\n",
    "def plot_losses(loss_train, loss_val):\n",
    "    plt.plot(loss_train, label='Training Loss')\n",
    "    plt.plot(loss_val, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BCE Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a8c3559",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_dim': 68, 'output_dim': 34, 'activation': 'relu'},\n",
       " {'input_dim': 34, 'output_dim': 17, 'activation': 'relu'},\n",
       " {'input_dim': 17, 'output_dim': 1, 'activation': 'sigmoid'}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a52c499a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,68) and (1,17) not aligned: 68 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate nueral network for classifer using best hyperparameters.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNeuralNetwork(nn_arch, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m, seed \u001b[38;5;241m=\u001b[39m seed, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m classifier_loss_train, classifier_loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/final-nn/nn/nn.py:353\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    350\u001b[0m     entry_epoch_loss_val\u001b[38;5;241m.\u001b[39mappend(loss_val)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# Perform backpropagation and update parameters.\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     grad_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_params(grad_dict)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Append mean epoch loss from training and validation.\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/final-nn/nn/nn.py:246\u001b[0m, in \u001b[0;36mNeuralNetwork.backprop\u001b[0;34m(self, y, y_hat, cache)\u001b[0m\n\u001b[1;32m    237\u001b[0m W_curr, b_curr, Z_curr, A_prev, dA_curr, activation_curr \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m                                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    239\u001b[0m                                                             cache[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m                                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march[L \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    243\u001b[0m )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Run single instance of backpropagation.\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m dA_prev, dW_curr, db_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_curr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mb_curr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mZ_curr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mdA_curr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mactivation_curr\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Store gradient information into dictionary.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m grad_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA_prev\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dA_prev\n",
      "File \u001b[0;32m~/GitHub/final-nn/nn/nn.py:202\u001b[0m, in \u001b[0;36mNeuralNetwork._single_backprop\u001b[0;34m(self, W_curr, b_curr, Z_curr, A_prev, dA_curr, activation_curr)\u001b[0m\n\u001b[1;32m    199\u001b[0m     dZ_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relu_backprop(dA_curr, Z_curr)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Calculate all relevant derivatives.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m dA_prev \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mdZ_curr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_curr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m dW_curr \u001b[38;5;241m=\u001b[39m (A_prev\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mdot(dZ_curr)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;66;03m# Needs to align with dimensions of self._param_dict[Wx]\u001b[39;00m\n\u001b[1;32m    204\u001b[0m db_curr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ_curr, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(b_curr\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# Ensure that has same dimensions as bias.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,68) and (1,17) not aligned: 68 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Generate nueral network for classifer using best hyperparameters.\n",
    "classifier = nn.NeuralNetwork(nn_arch, lr = 0.0001, seed = seed, batch_size = 10, epochs = 500, loss_function = \"mse\")\n",
    "classifier_loss_train, classifier_loss_val = classifier.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses side by side.\n",
    "plot_losses(classifier_loss_train, classifier_loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc49381",
   "metadata": {},
   "source": [
    "### Quantify accuracy of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier.predict(X_test)\n",
    "prediction_error = mean_squared_error(y_test, prediction)\n",
    "print(\"Average classifier prediction error: \", prediction_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
